{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro import distributions\n",
    "import pyro, torch\n",
    "import numpy as np\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "sys.path += [\"../src\"]\n",
    "import BC_roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = BC_roles.simulator_opinion_dynamics(BC_roles.create_edges_BC_BF_roles, BC_roles.simulator_opinion_dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "T = 32\n",
    "edge_per_t = 5\n",
    "epsilon_plus = (0.2, 0.4)\n",
    "epsilon_minus = (0.9, 0.7)\n",
    "mu_plus = (0.3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.initialize_simulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assume to observe both the roles, the opinions, and the interactions\n",
    "\n",
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self,dim,\n",
    "                      n_flows,\n",
    "                     base_dist=lambda dim:distributions.Uniform(torch.zeros(dim), torch.ones(dim)),\n",
    "                     flow_type=lambda kwargs:distributions.transforms.RadialFlow(**kwargs),\n",
    "                     args={'flow_args':{'dim':2}}):\n",
    "        super(NormalizingFlow, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n_flows = n_flows\n",
    "        self.base_dist = base_dist(dim)\n",
    "        self.uuid = np.random.randint(low=0,high=10000,size=1)[0]\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        If the flow needs an autoregressive net, build it for every flow\n",
    "        \"\"\"\n",
    "        if 'arn_hidden' in args:\n",
    "            self.arns = nn.ModuleList([AutoRegressiveNN(dim,\n",
    "                                                        args['arn_hidden'],\n",
    "                                                        param_dims=[self.dim]*args['n_params']) for _ in range(n_flows)])\n",
    "    \n",
    "        \"\"\"\n",
    "        Initialize all flows\n",
    "        \"\"\"\n",
    "        self.nfs = []\n",
    "        for f in range(n_flows):\n",
    "            if 'autoregressive_nn' in args['flow_args']:\n",
    "                args['flow_args']['autoregressive_nn'] = self.arns[f]\n",
    "            nf = flow_type(args['flow_args'])\n",
    "            self.nfs.append(nf)\n",
    "\n",
    "        \"\"\"\n",
    "        This step assumes that nfs={f_i}_{i=1}^N and that base_dist=N(0,I)\n",
    "        Then, register the (biejctive) transformation Z=nfs(eps), eps~base_dist\n",
    "        \"\"\"\n",
    "        self.nf_dist = distributions.TransformedDistribution(self.base_dist, self.nfs)\n",
    "        \n",
    "        self._register()\n",
    "        \n",
    "    def _register(self):\n",
    "        \"\"\"\n",
    "        Register all N flows with Pyro\n",
    "        \"\"\"\n",
    "        for f in range(self.n_flows):\n",
    "            nf_module = pyro.module(\"%d_nf_%d\" %(self.uuid,f), self.nfs[f])\n",
    "\n",
    "    \n",
    "    def target(self,X,roles,u,v,s_plus_, s_minus_,t,rho):\n",
    "        \n",
    "        dist = self.nf_dist\n",
    "        theta = pyro.sample(\"latent\", dist)\n",
    "        epsilon_plus, epsilon_minus = torch.sigmoid(theta) / 2\n",
    "        \n",
    "        diff_X = X[t,u] - X[t,v]\n",
    "        roles_u = roles[u]\n",
    "        kappas_plus = BC_roles.kappa_plus_from_epsilon(epsilon_plus[roles_u], diff_X, rho)\n",
    "        kappas_minus = BC_roles.kappa_plus_from_epsilon(epsilon_minus[roles_u], diff_X, rho)\n",
    "\n",
    "        \n",
    "        with pyro.plate(\"data\", s_plus_.shape[0]):\n",
    "            pyro.sample(\"obs_plus\", distributions.Bernoulli(probs = kappas_plus), obs = s_plus)\n",
    "            pyro.sample(\"obs_minus\", distributions.Bernoulli(probs = kappas_minus), obs = s_minus)\n",
    "\n",
    "\n",
    "    def model(self,X,roles,u,v,s_plus_, s_minus_,t,rho):\n",
    "        \"\"\"\n",
    "        q(z|x), once again x is not required\n",
    "        \n",
    "        1. Sample Z ~ nfs(eps), eps ~ N(0,I)\n",
    "        \n",
    "        This is the NN being trained\n",
    "        \"\"\"\n",
    "        self._register()\n",
    "        # print(\"nf_event\", self.nf_dist.event_shape)\n",
    "        # print(\"nf_batch\", self.nf_dist.batch_shape)\n",
    "        pyro.sample(\"latent\", self.nf_dist)\n",
    "\n",
    "    def sample(self,n):\n",
    "        \"\"\"\n",
    "        Sample a batch of (n,dim)\n",
    "        \n",
    "        Bug: in IAF and IAFStable, the dimensions throw an error (todo)\n",
    "        \"\"\"\n",
    "        return self.nf_dist.sample(torch.Size([n]))\n",
    "    \n",
    "    def log_prob(self,z):\n",
    "        \"\"\"\n",
    "        Returns log q(z|x) for z (assuming no x is required)\n",
    "        \"\"\"\n",
    "        return self.nf_dist.log_prob(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = BC_roles.simulator_opinion_dynamics(BC_roles.create_edges_BC_BF_roles, BC_roles.opinion_update_BC_BF_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "flow = distributions.transforms.Sylvester #2000 epochs. loss 135\n",
    "base_dist_n = lambda dim:distributions.Normal(torch.zeros(dim), torch.ones(dim)) \n",
    "base_dist_u = lambda dim:distributions.Uniform(torch.zeros(dim), torch.ones(dim)) \n",
    "\n",
    "dim = 1\n",
    "n_flows = 4\n",
    "\n",
    "if flow.__name__ in ['Sylvester']:\n",
    "    args = {'flow_args':{'input_dim':dim,\n",
    "                         'count_transforms':8}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist_s = lambda dim:distributions.Normal(torch.zeros(dim), torch.ones(dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_SVI_bc_roles(X, edges, roles, n_flows = 4, n_steps = 200, base_dist = \"normal\", mu_plus, mu_minus, rho):\n",
    "    dim = torc.Tensor([2,2])\n",
    "\n",
    "    if base_dist == \"normal\":\n",
    "        base_dist_s = lambda dim:distributions.Normal(torch.zeros(dim), torch.ones(dim))\n",
    "    elif base_dist == \"uniform\":\n",
    "        base_dist_s = lambda dim:distributions.Uniform(torch.zeros(dim), torch.ones(dim))  ][base_dist_sample]\n",
    "\n",
    "    u,v,s_plus,s_minus = convert_edges_uvst(edges)\n",
    "    s_plus_ = s_plus.to(torch.float32)\n",
    "    s_minus_ = s_minus.to(torch.float32)\n",
    "\n",
    "    nf_obj = NormalizingFlow(dim=dim,\n",
    "                             n_flows=n_flows,\n",
    "                             base_dist=base_dist_s,\n",
    "                             flow_type=lambda kwargs:flow(**kwargs),\n",
    "                             args=args)\n",
    "\n",
    "    adam_params = {\"lr\": 0.05, \"betas\": (0.90, 0.999)}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    t0 = time()\n",
    "\n",
    "    # setup the inference algorithm\n",
    "    svi = SVI(nf_obj.target, nf_obj.model, optimizer, loss=Trace_ELBO())\n",
    "    # dist = p_z() # true distribution\n",
    "    # do gradient steps\n",
    "    losses = []\n",
    "    for step in range(n_steps):\n",
    "        # data = dist.rsample(torch.Size([128])) # using a batch of 128 new data points every step\n",
    "        loss = svi.step(X,u,v,s_plus_, s_minus_,t,rho) # analogous to opt.step() in PyTorch\n",
    "        losses.append(loss)\n",
    "        # if step % 100 == 0:\n",
    "        #     print(loss)\n",
    "    t1 = time()\n",
    "\n",
    "    tot_time = t1 - t0\n",
    "\n",
    "    return nf_obj, tot_time\n",
    "\n",
    "def convert_edges_uvst(edges):\n",
    "    max_T, edge_per_t, num_s = edges.size()\n",
    "    \n",
    "    uvst = torch.cat((edges.reshape(((max_T) * edge_per_t, num_s)), torch.Tensor(np.repeat(np.arange(max_T), edge_per_t))[:, None]), dim = 1).T.long()\n",
    "    return uvst\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
